#!/usr/bin/env python
from __future__ import print_function
import argparse
import datetime
import imp
import json
import logging
import sys
import time

import stat_daemon.plugins.stats
import stat_daemon.tsa

from airflow.executors import SequentialExecutor, LocalExecutor, CeleryExecutor
from stat_daemon.util import require_args, load_plugin
from stat_daemon.models import MetadataTable, MetadataTags, MetadataAlerts
from stat_daemon.tasks import TaskList, TaskRunner

from stat_daemon.captain_hindsight.www.app import CaptainHindsight

def status(args):
    """
    Report on the status of a specified data set
    """
    pass

def alert(args):
    """
    Send an alert for a dataset
    """
    pass

def webserver(args):
    """
    Start the webserver
    """
    app = CaptainHindsight(table=args.dest,
            sql_conn_id=args.sql_conn_id).get_app()
    logging.getLogger().setLevel(logging.INFO)
    app.run(debug=args.debug, host='0.0.0.0', port=args.port)


def detrend_timeseries(args):
    """
    Remove trends
    """
    require_args(args, {'type', 'path'})
    path = args.type + ':' + args.path
    data, fit, error, tags = stat_daemon.tsa.detrend(
                path, args.stat, 
                table=args.dest, 
                sql_conn_id=args.sql_conn_id,
                start_time=args.start_time,
                end_time=args.end_time,
                plugin=args.plugin)
    print(json.dumps([(x.isoformat(), y) for x, y in fit]))


def create_table(args):
    """
    Creates the stats table
    """
    table = MetadataTable(table_name=args.dest,
                          sql_conn_id=args.sql_conn_id)
    table.create_table(drop=args.drop, with_test_data=args.with_test_data)


def clear_stats(args):
    """
    Clear stats matching type and path
    """
    require_args(args, {'type', 'path'})
    logging.info("Deleting items like: {}:{}".format(args.type, args.path))
    table = MetadataTable(table_name=args.dest,
                          sql_conn_id=args.sql_conn_id)
    table.delete_records(args.path, args.type, args.filters)


def show_stats(args):
    """
    Show stats from stats table
    """
    table = MetadataTable(table_name=args.dest,
                          sql_conn_id=args.sql_conn_id)
    rows = table.get_records(where=args.filters,
                             limit=args.limit)
    print(rows)


def ls_tasks(args):
    """
    Prints tasks that will be performed
    """
    tl = TaskList(args)
    for task in tl.delete:
        print("Metadata will be deleted for: {}".format(task))
    for task in tl.update:
        print("Metadata will be updated for: {}".format(task))


def update(args):
    """
    Update stats
    """
    require_args(args, {'type', 'path'})
    plugin = None
    if args.plugin:
        plugin = load_plugin(args.plugin)
    else:
        try:
            logging.info("Plugin not specified; using default.")
            plugin = getattr(
                stat_daemon.plugins.stats, '{}_default'.format(args.type))
        except Exception as e:
            logging.error("Could not load plugin {}_default".format(args.type))
            logging.error(e)
            return
    try:
        meta = MetadataTable(args.dest, sql_conn_id=args.sql_conn_id)
        logging.info("Loading data into metadata table.")
        meta.insert_rows(plugin.get_rows(args))
    except Exception as e:
        logging.error("Failed to create rows.")
        logging.error(e)
        return


def test(args):
    """
    Test runs with local executor
    """
    runner = TaskRunner(LocalExecutor(), args)
    runner.run()


def start(args):
    """
    Run in production mode.  

    This currently runs a pseudo-daemon process (infite while loop)
    """
    executor = None
    if args.executor == 'local':
        executor = LocalExecutor(parallelism=args.parallelism)
    elif args.executor == 'celery':
        executor = CeleryExecutor(parallelism=args.parallelism)
    else:
        logging.error("Unrecognized executor: {}".format(args.executor))
        sys.exit(1)
    while True:
        runner = TaskRunner(executor, args)
        runner.run()
        time.sleep(args.sleep)


def version(args):
    """
    Prints the version
    """
    print("1.0")


def add_args(parser, args=None):
    """
    Add arguments to parser programmatically
    """
    if not args:
        return parser
    sql_conn_help = "Connection id of the stats database"
    metastore_conn_help = "Connection id of the hive metastore"
    hdfs_conn_help = "Connection id of HDFS"
    hive_conn_help = "Connection id of hive"
    presto_conn_help = "Connection id of presto"
    if 'sql_conn_id' in args or 'all_conn' in args:
        parser.add_argument("--sql_conn_id",
                            help=sql_conn_help,
                            default="airflow_db")
    if 'metastore_conn_id' in args or 'all_conn' in args:
        parser.add_argument("--metastore_conn_id",
                            help=metastore_conn_help,
                            default="metastore_default")
    if 'metastore_mysql_conn_id' in args or 'all_conn' in args:
        parser.add_argument("--metastore_mysql_conn_id",
                            help=metastore_conn_help,
                            default="metastore_mysql")
    if 'hdfs_conn_id' in args or 'all_conn' in args:
        parser.add_argument("--hdfs_conn_id",
                            help=hdfs_conn_help,
                            default="hdfs_default")
    if 'hiveserver2_conn_id' in args or 'all_conn' in args:
        parser.add_argument("--hiveserver2_conn_id",
                            help=hive_conn_help,
                            default="hiveserver2_default")
    if 'presto_conn_id' in args or 'all_conn' in args:
        parser.add_argument("--presto_conn_id",
                            help=presto_conn_help,
                            default="presto_default")
    if 'dest' in args:
        parser.add_argument("--dest",
                            default="metadata",
                            help="Table to write stats to")
    if 'filters' in args:
        parser.add_argument("--filters",
                             help="Where clause")

    if 'taskfolder' in args:
        parser.add_argument("--taskfolder",
                            nargs='+',
                            help="Path to folder(s) containing taskfiles")
    if 'sleep' in args:
        parser.add_argument("--sleep",
                            type=int,
                            default=1800,
                            help="Time to sleep for")
    if 'parallelism' in args:
        parser.add_argument("--parallelism",
                            type=int,
                            default=10,
                            help="Max number of simultaneous jobs to run")
    if 'maxjobs' in args:
        parser.add_argument("--maxjobs",
                            type=int,
                            default=10000,
                            help="Max number of jobs to enqeue")
    if 'executor' in args:
        parser.add_argument("--executor",
                            help="Executor type",
                            default='local',
                            choices=['local', 'celery'])
    if 'type' in args:
        parser.add_argument("--type",
                            help="Data type")
    if 'path' in args:
        parser.add_argument("--path",
                            help="Data path")
    if 'stat' in args:
        parser.add_argument("--stat",
                            help="Stat to consider")
    if 'plugin' in args:
        parser.add_argument("--plugin",
                            help="Plugin script to be executed")
        parser.add_argument("--plugin_args",
                            help="Extra args to pass to the plugin")
    return parser


def get_parser():
    """
    Returns parse arguements
    """
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')
    ht = "Create table"
    parser_create = subparsers.add_parser('create_table', help=ht)
    parser_create.add_argument("--drop",
                               action='store_true',
                               help="Drop and re-create the table")
    parser_create.add_argument("--with_test_data",
                               action='store_true',
                               help="Create the table with test data")
    parser_create = add_args(parser_create, {'dest', 'sql_conn_id'})
    parser_create.set_defaults(func=create_table)
    ht = "Clear stats matching a given pattern"
    parser_delete = subparsers.add_parser('clear_stats', help=ht)
    parser_delete = add_args(parser_delete,
                             {'type', 'path', 'dest', 'filters',
                             'sql_conn_id'})
    parser_delete.set_defaults(func=clear_stats)
    ht = "Detrend the data set"
    parser_detrend_timeseries = subparsers.add_parser('detrend_timeseries',
     help=ht)
    parser_detrend_timeseries.add_argument("--start_time",
                                default='2011-01-01',
                               help="earliest time to consider")
    parser_detrend_timeseries.add_argument("--end_time",
                                default=datetime.datetime.utcnow(
                                    ).strftime('%Y-%m-%d'),
                               help="The latest time to consider")
    parser_detrend_timeseries = add_args(parser_detrend_timeseries,
                           {'plugin', 'type', 'path', 'stat', 'dest',
                           'sql_conn_id'})
    parser_detrend_timeseries.set_defaults(func=detrend_timeseries)
    ht = "List the tasks that will be performed"
    parser_ls_tasks = subparsers.add_parser('ls_tasks', help=ht)
    parser_ls_tasks = add_args(parser_ls_tasks,
                               {'type', 'path', 'dest', 'all_conn'})
    parser_ls_tasks.set_defaults(func=ls_tasks)
    ht = "Run in production mode"
    parser_start = subparsers.add_parser('start', help=ht)
    parser_start = add_args(parser_start,
                            {'plugin', 'type', 'path', 'dest', 'taskfolder',
                             'sleep', 'all_conn', 'maxjobs',
                             'parallelism', 'executor'})
    parser_start.set_defaults(func=start)
    ht = "Show stats"
    parser_show = subparsers.add_parser('show_stats', help=ht)
    parser_show.add_argument("--limit",
                             type=int,
                             default=1000,
                             help="Limit clause")
    parser_show = add_args(parser_show, {'dest', 'filters', 'sql_conn_id'})
    parser_show.set_defaults(func=show_stats)
    ht = "Run in test mode (LocalExecutor, no parallelism)"
    parser_test = subparsers.add_parser('test', help=ht)
    parser_test = add_args(parser_test,
                           {'plugin', 'type', 'path', 'dest', 'taskfolder',
                            'all_conn', 'maxjobs'})
    parser_test.set_defaults(func=test)
    ht = "Update stats"
    parser_update = subparsers.add_parser('update', help=ht)
    parser_update = add_args(parser_update,
                             {'plugin', 'type', 'path', 'dest',
                             'all_conn'})
    parser_update.set_defaults(func=update)
    parser_version = subparsers.add_parser('version',
                                           help="Show version")
    parser_version.set_defaults(func=version)
    ht = "Start the webserver"
    parser_webserver = subparsers.add_parser('webserver', help=ht)
    parser_webserver.add_argument("--port",
                                    type=int,
                                    default=6969,
                                    help="The port to run on")
    parser_webserver.add_argument("--debug",
                                    action='store_true',
                                    help="Run in debug mode")
    parser_webserver = add_args(parser_webserver, {'dest', 'sql_conn_id'})
    parser_webserver.set_defaults(func=webserver)
    return parser

if __name__ == '__main__':
    """
    Run the main loop
    """
    logging.getLogger().setLevel(logging.INFO)
    parser = get_parser()
    args = parser.parse_args()
    args.func(args)
